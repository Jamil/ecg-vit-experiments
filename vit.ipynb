{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a1c6d5-5bb7-4417-a5a3-6c88c4c4110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import Compose, Lambda, Normalize, RandomHorizontalFlip, RandomResizedCrop, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794ad182-c5be-4ffc-ae8c-b41642f5f55e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    IMAGE_PROCESSOR_MAPPING,\n",
    "    MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForMaskedImageModeling,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1ecd6d-e7ca-488c-b185-04eeceb05cbf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Pre-training a ðŸ¤— Transformers model for simple masked image modeling (SimMIM).\n",
    "Any model supported by the AutoModelForMaskedImageModeling API can be used.\n",
    "\"\"\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.36.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/image-pretraining/requirements.txt\")\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dacabf6-8a3a-43d3-a8a2-2833cb9bdcb7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    Using `HfArgumentParser` we can turn this class into argparse arguments to be able to\n",
    "    specify them on the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=\"cifar10\", metadata={\"help\": \"Name of a dataset from the datasets package\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    image_column_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The column name of the images in the files. If not set, will try to use 'image' or 'img'.\"},\n",
    "    )\n",
    "    train_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the training data.\"})\n",
    "    validation_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the validation data.\"})\n",
    "    train_val_split: Optional[float] = field(\n",
    "        default=0.15, metadata={\"help\": \"Percent to split off of train for validation.\"}\n",
    "    )\n",
    "    mask_patch_size: int = field(default=32, metadata={\"help\": \"The size of the square patches to use for masking.\"})\n",
    "    mask_ratio: float = field(\n",
    "        default=0.6,\n",
    "        metadata={\"help\": \"Percentage of patches to mask.\"},\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        data_files = {}\n",
    "        if self.train_dir is not None:\n",
    "            data_files[\"train\"] = self.train_dir\n",
    "        if self.validation_dir is not None:\n",
    "            data_files[\"val\"] = self.validation_dir\n",
    "        self.data_files = data_files if data_files else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ad508d-c742-4064-9449-1aa443024a3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/image processor we are going to pre-train.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization. Can be a local path to a pytorch_model.bin or a \"\n",
    "                \"checkpoint identifier on the hub. \"\n",
    "                \"Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name_or_path: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    config_overrides: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override some existing default config settings when a model is trained from scratch. Example: \"\n",
    "                \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store (cache) the pretrained models/datasets downloaded from the hub\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    image_processor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n",
    "    token: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n",
    "                \"generated when running `huggingface-cli login` (stored in `~/.huggingface`).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\"\n",
    "        },\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\"\n",
    "                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n",
    "                \"execute code present on the Hub on your local machine.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    image_size: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The size (resolution) of each image. If not specified, will use `image_size` of the configuration.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    patch_size: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The size (resolution) of each patch. If not specified, will use `patch_size` of the configuration.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    encoder_stride: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Stride to use for the encoder.\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b28ca77-8c48-4d34-9271-ef8b5a7f99ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MaskGenerator:\n",
    "    \"\"\"\n",
    "    A class to generate boolean masks for the pretraining task.\n",
    "\n",
    "    A mask is a 1D tensor of shape (model_patch_size**2,) where the value is either 0 or 1,\n",
    "    where 1 indicates \"masked\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6):\n",
    "        self.input_size = input_size\n",
    "        self.mask_patch_size = mask_patch_size\n",
    "        self.model_patch_size = model_patch_size\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        if self.input_size % self.mask_patch_size != 0:\n",
    "            raise ValueError(\"Input size must be divisible by mask patch size\")\n",
    "        if self.mask_patch_size % self.model_patch_size != 0:\n",
    "            raise ValueError(\"Mask patch size must be divisible by model patch size\")\n",
    "\n",
    "        self.rand_size = self.input_size // self.mask_patch_size\n",
    "        self.scale = self.mask_patch_size // self.model_patch_size\n",
    "\n",
    "        self.token_count = self.rand_size**2\n",
    "        self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))\n",
    "\n",
    "    def __call__(self):\n",
    "        mask_idx = np.random.permutation(self.token_count)[: self.mask_count]\n",
    "        mask = np.zeros(self.token_count, dtype=int)\n",
    "        mask[mask_idx] = 1\n",
    "\n",
    "        mask = mask.reshape((self.rand_size, self.rand_size))\n",
    "        mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1)\n",
    "\n",
    "        return torch.tensor(mask.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f2b8d31-23fe-406e-aa0e-a3febdf6bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    mask = torch.stack([example[\"mask\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"bool_masked_pos\": mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8981fc9-7ed0-4480-9869-f90e93ff54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"model_type\": \"vit\",\n",
    "    \"output_dir\": \"./outputs/\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"label_names\": \"bool_masked_pos\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"num_train_epochs\": 100,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"logging_steps\": 10,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"seed\": 1337\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c6dc1a-66ed-4976-ad87-8430748e491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_dict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31efaa7b-36d1-41fb-8a5e-ec1d43c44899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArguments(model_name_or_path=None, model_type='vit', config_name_or_path=None, config_overrides=None, cache_dir=None, model_revision='main', image_processor_name=None, token=None, use_auth_token=None, trust_remote_code=False, image_size=None, patch_size=None, encoder_stride=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95f29baa-3d60-4b8b-86d0-d17d158ad884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf76aee3-b35d-4289-adb6-bbfdc5f01221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/30/2023 17:35:40 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/30/2023 17:35:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=bool_masked_pos,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./outputs/runs/Nov30_17-34-59_DNa226c07.SUNet,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=100,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "output_dir=./outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./outputs/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.EPOCH,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.05,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n",
    "    + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86919806-2fd2-47bf-95f1-c90496474d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                             [--model_type MODEL_TYPE]\n",
      "                             [--config_name_or_path CONFIG_NAME_OR_PATH]\n",
      "                             [--config_overrides CONFIG_OVERRIDES]\n",
      "                             [--cache_dir CACHE_DIR]\n",
      "                             [--model_revision MODEL_REVISION]\n",
      "                             [--image_processor_name IMAGE_PROCESSOR_NAME]\n",
      "                             [--token TOKEN]\n",
      "                             [--use_auth_token [USE_AUTH_TOKEN]]\n",
      "                             [--trust_remote_code [TRUST_REMOTE_CODE]]\n",
      "                             [--image_size IMAGE_SIZE]\n",
      "                             [--patch_size PATCH_SIZE]\n",
      "                             [--encoder_stride ENCODER_STRIDE]\n",
      "                             [--dataset_name DATASET_NAME]\n",
      "                             [--dataset_config_name DATASET_CONFIG_NAME]\n",
      "                             [--image_column_name IMAGE_COLUMN_NAME]\n",
      "                             [--train_dir TRAIN_DIR]\n",
      "                             [--validation_dir VALIDATION_DIR]\n",
      "                             [--train_val_split TRAIN_VAL_SPLIT]\n",
      "                             [--mask_patch_size MASK_PATCH_SIZE]\n",
      "                             [--mask_ratio MASK_RATIO]\n",
      "                             [--max_train_samples MAX_TRAIN_SAMPLES]\n",
      "                             [--max_eval_samples MAX_EVAL_SAMPLES]\n",
      "                             --output_dir OUTPUT_DIR\n",
      "                             [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                             [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                             [--do_predict [DO_PREDICT]]\n",
      "                             [--evaluation_strategy {no,steps,epoch}]\n",
      "                             [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                             [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                             [--eval_delay EVAL_DELAY]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau}]\n",
      "                             [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]\n",
      "                             [--warmup_ratio WARMUP_RATIO]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--log_level {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "                             [--no_log_on_each_node]\n",
      "                             [--logging_dir LOGGING_DIR]\n",
      "                             [--logging_strategy {no,steps,epoch}]\n",
      "                             [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "                             [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
      "                             [--no_logging_nan_inf_filter]\n",
      "                             [--save_strategy {no,steps,epoch}]\n",
      "                             [--save_steps SAVE_STEPS]\n",
      "                             [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                             [--save_safetensors [SAVE_SAFETENSORS]]\n",
      "                             [--no_save_safetensors]\n",
      "                             [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "                             [--save_only_model [SAVE_ONLY_MODEL]]\n",
      "                             [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]\n",
      "                             [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n",
      "                             [--data_seed DATA_SEED]\n",
      "                             [--jit_mode_eval [JIT_MODE_EVAL]]\n",
      "                             [--use_ipex [USE_IPEX]] [--bf16 [BF16]]\n",
      "                             [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                             [--half_precision_backend {auto,apex,cpu_amp}]\n",
      "                             [--bf16_full_eval [BF16_FULL_EVAL]]\n",
      "                             [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--ddp_backend {nccl,gloo,mpi,ccl,hccl}]\n",
      "                             [--tpu_num_cores TPU_NUM_CORES]\n",
      "                             [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
      "                             [--debug DEBUG [DEBUG ...]]\n",
      "                             [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                             [--eval_steps EVAL_STEPS]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                             [--disable_tqdm DISABLE_TQDM]\n",
      "                             [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                             [--no_remove_unused_columns]\n",
      "                             [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                             [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                             [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                             [--greater_is_better GREATER_IS_BETTER]\n",
      "                             [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
      "                             [--fsdp FSDP]\n",
      "                             [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
      "                             [--fsdp_config FSDP_CONFIG]\n",
      "                             [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "                             [--deepspeed DEEPSPEED]\n",
      "                             [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                             [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop}]\n",
      "                             [--optim_args OPTIM_ARGS]\n",
      "                             [--adafactor [ADAFACTOR]]\n",
      "                             [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                             [--length_column_name LENGTH_COLUMN_NAME]\n",
      "                             [--report_to REPORT_TO [REPORT_TO ...]]\n",
      "                             [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                             [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "                             [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
      "                             [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "                             [--no_dataloader_pin_memory]\n",
      "                             [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "                             [--no_skip_memory_metrics]\n",
      "                             [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
      "                             [--push_to_hub [PUSH_TO_HUB]]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--hub_private_repo [HUB_PRIVATE_REPO]]\n",
      "                             [--hub_always_push [HUB_ALWAYS_PUSH]]\n",
      "                             [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "                             [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n",
      "                             [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
      "                             [--fp16_backend {auto,apex,cpu_amp}]\n",
      "                             [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
      "                             [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "                             [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
      "                             [--mp_parameters MP_PARAMETERS]\n",
      "                             [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "                             [--full_determinism [FULL_DETERMINISM]]\n",
      "                             [--torchdynamo TORCHDYNAMO]\n",
      "                             [--ray_scope RAY_SCOPE]\n",
      "                             [--ddp_timeout DDP_TIMEOUT]\n",
      "                             [--torch_compile [TORCH_COMPILE]]\n",
      "                             [--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
      "                             [--torch_compile_mode TORCH_COMPILE_MODE]\n",
      "                             [--dispatch_batches DISPATCH_BATCHES]\n",
      "                             [--split_batches [SPLIT_BATCHES]]\n",
      "                             [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n",
      "                             [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n",
      "                             [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --output_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "\n",
    "# Initialize our dataset.\n",
    "ds = load_dataset(\n",
    "    data_args.dataset_name,\n",
    "    data_args.dataset_config_name,\n",
    "    data_files=data_args.data_files,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    token=model_args.token,\n",
    ")\n",
    "\n",
    "# If we don't have a validation split, split off a percentage of train as validation.\n",
    "data_args.train_val_split = None if \"validation\" in ds.keys() else data_args.train_val_split\n",
    "if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n",
    "    split = ds[\"train\"].train_test_split(data_args.train_val_split)\n",
    "    ds[\"train\"] = split[\"train\"]\n",
    "    ds[\"validation\"] = split[\"test\"]\n",
    "\n",
    "# Create config\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"token\": model_args.token,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "}\n",
    "if model_args.config_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name_or_path, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "else:\n",
    "    config = CONFIG_MAPPING[model_args.model_type]()\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "    if model_args.config_overrides is not None:\n",
    "        logger.info(f\"Overriding config: {model_args.config_overrides}\")\n",
    "        config.update_from_string(model_args.config_overrides)\n",
    "        logger.info(f\"New config: {config}\")\n",
    "\n",
    "# make sure the decoder_type is \"simmim\" (only relevant for BEiT)\n",
    "if hasattr(config, \"decoder_type\"):\n",
    "    config.decoder_type = \"simmim\"\n",
    "\n",
    "# adapt config\n",
    "model_args.image_size = model_args.image_size if model_args.image_size is not None else config.image_size\n",
    "model_args.patch_size = model_args.patch_size if model_args.patch_size is not None else config.patch_size\n",
    "model_args.encoder_stride = (\n",
    "    model_args.encoder_stride if model_args.encoder_stride is not None else config.encoder_stride\n",
    ")\n",
    "\n",
    "config.update(\n",
    "    {\n",
    "        \"image_size\": model_args.image_size,\n",
    "        \"patch_size\": model_args.patch_size,\n",
    "        \"encoder_stride\": model_args.encoder_stride,\n",
    "    }\n",
    ")\n",
    "\n",
    "# create image processor\n",
    "if model_args.image_processor_name:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(model_args.image_processor_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "else:\n",
    "    IMAGE_PROCESSOR_TYPES = {\n",
    "        conf.model_type: image_processor_class for conf, image_processor_class in IMAGE_PROCESSOR_MAPPING.items()\n",
    "    }\n",
    "    image_processor = IMAGE_PROCESSOR_TYPES[model_args.model_type]()\n",
    "\n",
    "# create model\n",
    "if model_args.model_name_or_path:\n",
    "    model = AutoModelForMaskedImageModeling.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        token=model_args.token,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelForMaskedImageModeling.from_config(config, trust_remote_code=model_args.trust_remote_code)\n",
    "\n",
    "if training_args.do_train:\n",
    "    column_names = ds[\"train\"].column_names\n",
    "else:\n",
    "    column_names = ds[\"validation\"].column_names\n",
    "\n",
    "if data_args.image_column_name is not None:\n",
    "    image_column_name = data_args.image_column_name\n",
    "elif \"image\" in column_names:\n",
    "    image_column_name = \"image\"\n",
    "elif \"img\" in column_names:\n",
    "    image_column_name = \"img\"\n",
    "else:\n",
    "    image_column_name = column_names[0]\n",
    "\n",
    "# transformations as done in original SimMIM paper\n",
    "# source: https://github.com/microsoft/SimMIM/blob/main/data/data_simmim.py\n",
    "transforms = Compose(\n",
    "    [\n",
    "        Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "        RandomResizedCrop(model_args.image_size, scale=(0.67, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create mask generator\n",
    "mask_generator = MaskGenerator(\n",
    "    input_size=model_args.image_size,\n",
    "    mask_patch_size=data_args.mask_patch_size,\n",
    "    model_patch_size=model_args.patch_size,\n",
    "    mask_ratio=data_args.mask_ratio,\n",
    ")\n",
    "\n",
    "def preprocess_images(examples):\n",
    "    \"\"\"Preprocess a batch of images by applying transforms + creating a corresponding mask, indicating\n",
    "    which patches to mask.\"\"\"\n",
    "\n",
    "    examples[\"pixel_values\"] = [transforms(image) for image in examples[image_column_name]]\n",
    "    examples[\"mask\"] = [mask_generator() for i in range(len(examples[image_column_name]))]\n",
    "\n",
    "    return examples\n",
    "\n",
    "if training_args.do_train:\n",
    "    if \"train\" not in ds:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    if data_args.max_train_samples is not None:\n",
    "        ds[\"train\"] = ds[\"train\"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))\n",
    "    # Set the training transforms\n",
    "    ds[\"train\"].set_transform(preprocess_images)\n",
    "\n",
    "if training_args.do_eval:\n",
    "    if \"validation\" not in ds:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        ds[\"validation\"] = (\n",
    "            ds[\"validation\"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))\n",
    "        )\n",
    "    # Set the validation transforms\n",
    "    ds[\"validation\"].set_transform(preprocess_images)\n",
    "\n",
    "# Initialize our trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"] if training_args.do_train else None,\n",
    "    eval_dataset=ds[\"validation\"] if training_args.do_eval else None,\n",
    "    tokenizer=image_processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()\n",
    "    trainer.log_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# Write model card and (optionally) push to hub\n",
    "kwargs = {\n",
    "    \"finetuned_from\": model_args.model_name_or_path,\n",
    "    \"tasks\": \"masked-image-modeling\",\n",
    "    \"dataset\": data_args.dataset_name,\n",
    "    \"tags\": [\"masked-image-modeling\"],\n",
    "}\n",
    "if training_args.push_to_hub:\n",
    "    trainer.push_to_hub(**kwargs)\n",
    "else:\n",
    "    trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5868511-0028-4dc3-8f8b-e055e7361c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
